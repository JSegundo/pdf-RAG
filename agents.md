# AI Assistant Guide for PDF-RAG Project

This document provides comprehensive information for AI assistants working with the PDF-RAG codebase. It contains technical details, architecture patterns, and development guidelines.

## ğŸ—ï¸ System Architecture

### Service Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js       â”‚    â”‚   Node.js       â”‚    â”‚   Python        â”‚
â”‚   Client        â”‚â—„â”€â”€â–ºâ”‚   API Server    â”‚â—„â”€â”€â–ºâ”‚   Processing    â”‚
â”‚   (Port 3500)   â”‚    â”‚   (Port 3000)   â”‚    â”‚   Service       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   (Port 8000)   â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   PostgreSQL    â”‚
                                              â”‚   + pgvector    â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   RabbitMQ      â”‚
                                              â”‚   (Message      â”‚
                                              â”‚    Queue)       â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Upload**: Client â†’ API Server â†’ RabbitMQ Queue
2. **Processing**: RabbitMQ â†’ Processing Service â†’ PostgreSQL
3. **Chat**: Client â†’ API Server â†’ Processing Service â†’ LLM â†’ Client

## ğŸ“ Project Structure

```
pdf-RAG/
â”œâ”€â”€ client/                    # Next.js frontend
â”‚   â”œâ”€â”€ src/app/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ PDFDropzone.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessingStatus.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ui/           # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ utils/            # Utility functions
â”‚   â”‚   â””â”€â”€ page.tsx          # Main page
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ server/                   # Node.js API server
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ controllers/  # Request handlers
â”‚   â”‚   â”‚   â””â”€â”€ routes/       # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/         # Chat management
â”‚   â”‚   â”‚   â”œâ”€â”€ llm/          # LLM providers
â”‚   â”‚   â”‚   â”œâ”€â”€ queue/        # Message queue
â”‚   â”‚   â”‚   â””â”€â”€ websocket/    # WebSocket handling
â”‚   â”‚   â”œâ”€â”€ middleware/       # Express middleware
â”‚   â”‚   â”œâ”€â”€ types/           # TypeScript types
â”‚   â”‚   â””â”€â”€ index.ts         # Server entry point
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ processing-service/       # Python processing service
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/             # FastAPI endpoints
â”‚   â”‚   â”œâ”€â”€ process_pipeline/ # Document processing
â”‚   â”‚   â”‚   â”œâ”€â”€ extract.py   # Text extraction (Docling)
â”‚   â”‚   â”‚   â”œâ”€â”€ chunk.py     # Text chunking
â”‚   â”‚   â”‚   â””â”€â”€ embed.py     # Embedding generation
â”‚   â”‚   â”œâ”€â”€ rag/             # RAG components
â”‚   â”‚   â”‚   â””â”€â”€ search.py    # Vector search
â”‚   â”‚   â”œâ”€â”€ storage/         # Database management
â”‚   â”‚   â”œâ”€â”€ notifier/        # Status notifications
â”‚   â”‚   â””â”€â”€ main.py          # Service entry point
â”‚   â””â”€â”€ requirements/
â””â”€â”€ docker-compose.dev.yml   # Development setup
```

## ğŸ”§ Key Technologies

### Frontend (Client)
- **Framework**: Next.js 15 with App Router
- **Language**: TypeScript
- **Styling**: Tailwind CSS
- **Components**: Radix UI primitives
- **File Handling**: react-dropzone
- **State Management**: React hooks (useState, useEffect)

### Backend (Server)
- **Runtime**: Node.js with TypeScript
- **Framework**: Express.js
- **WebSocket**: ws library
- **Queue**: amqplib (RabbitMQ client)
- **File Upload**: multer
- **LLM Integration**: @anthropic-ai/sdk, openai

### Processing Service
- **Framework**: FastAPI
- **Document Processing**: Docling 2.24.0
- **Embeddings**: OpenAI text-embedding-3-large
- **Database**: psycopg2-binary, pgvector
- **Queue**: pika (RabbitMQ client)
- **Async Processing**: Threading

### Infrastructure
- **Database**: PostgreSQL with pgvector extension
- **Message Queue**: RabbitMQ
- **Containerization**: Docker & Docker Compose
- **Database Admin**: PgAdmin

## ğŸ—„ï¸ Database Schema

### Tables
```sql
-- Documents metadata
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    filename TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Text chunks with embeddings
CREATE TABLE chunks (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES documents(id),
    chunk_text TEXT,
    embedding vector(1536),  -- OpenAI embedding dimension
    page_numbers INTEGER[],
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Vector similarity index
CREATE INDEX chunks_embedding_idx 
ON chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);
```

## ğŸ”„ Processing Pipeline

### Document Processing Flow
1. **Upload**: File uploaded to `/api/document/upload`
2. **Queue**: Job queued in RabbitMQ with metadata
3. **Extraction**: Docling extracts text and structure
4. **Chunking**: Text split into optimal chunks (configurable size)
5. **Embedding**: OpenAI generates 1536-dimension vectors
6. **Storage**: Chunks stored in PostgreSQL with metadata
7. **Notification**: WebSocket notifies completion

### Chat Processing Flow
1. **Query**: User message sent to `/api/chat/chat`
2. **Embedding**: Query converted to vector
3. **Search**: Vector similarity search in PostgreSQL
4. **Context**: Top-k results combined with conversation history
5. **Generation**: LLM generates response with context
6. **Response**: Answer returned to client

## ğŸ› ï¸ Development Patterns

### Error Handling
- **Server**: Express error middleware with structured responses
- **Processing**: Try-catch with retry logic and queue rejection
- **Client**: Error boundaries and user-friendly messages

### Logging
- **Server**: Console logging with structured format
- **Processing**: Python logging with configurable levels
- **Docker**: JSON file logging with rotation

### Configuration
- **Environment Variables**: All secrets and config via .env
- **Docker**: Environment variables in docker-compose
- **TypeScript**: Strong typing for all interfaces

## ğŸ” Key Classes and Functions

### Server (Node.js)

#### ChatManager
```typescript
class ChatManager {
  // Handles chat flow orchestration
  async handleMessage(request: ChatRequest): Promise<ChatResponse>
  private async getVectorSearchResults(query: string): Promise<VectorSearchResult[]>
  private buildContext(searchResults: VectorSearchResult[], conversationId: string): string
  private async generateResponse(context: string, userMessage: string): Promise<string>
}
```

#### LLMService
```typescript
class LLMService {
  // Multi-provider LLM integration
  async generateResponse(request: LLMRequest): Promise<LLMResponse>
  setProvider(provider: 'anthropic' | 'openai'): void
}
```

### Processing Service (Python)

#### DocumentProcessor
```python
class DocumentProcessor:
    # Main processing pipeline
    def process_document(self, file_id: str, file_path: str, metadata: Dict = None) -> Dict
```

#### VectorSearch
```python
class VectorSearch:
    # Vector similarity search
    def search(self, query: str, document_id: Optional[int] = None, 
               top_k: int = 5, min_score: float = 0.0) -> List[Dict[str, Any]]
    def _generate_embedding(self, text: str) -> List[float]
```

## ğŸš¨ Common Issues and Solutions

### Memory Issues
- **Problem**: Processing service OOM with large documents
- **Solution**: Increase memory limits in docker-compose.yml
- **Monitoring**: `docker stats` to check memory usage

### Queue Connection Issues
- **Problem**: RabbitMQ connection drops during processing
- **Solution**: Implement connection retry logic
- **Monitoring**: Check RabbitMQ management interface

### Database Connection Issues
- **Problem**: PostgreSQL connection pool exhaustion
- **Solution**: Implement connection pooling and proper cleanup
- **Monitoring**: Check database connection count

### WebSocket Issues
- **Problem**: WebSocket connections drop during long processing
- **Solution**: Implement heartbeat and reconnection logic
- **Monitoring**: Check WebSocket connection status

## ğŸ”§ Development Commands

### Local Development
```bash
# Start infrastructure
docker-compose -f docker-compose.dev.yml up postgres rabbitmq pgadmin

# Start API server
cd server && npm run dev

# Start processing service
cd processing-service && python src/main.py

# Start client
cd client && npm run dev
```

### Docker Development
```bash
# Full development environment
npm run dev

# Production build
npm run build && npm start

# View logs
npm run logs
```

### Database Operations
```bash
# Connect to database
docker exec -it pdf_chat_rag-postgres-1 psql -U postgres -d ragdb

# Check vector extension
\dx

# View tables
\dt

# Query chunks
SELECT COUNT(*) FROM chunks;
```

## ğŸ“Š Performance Considerations

### Memory Usage
- **Docling Models**: ~6.5GB for OCR models (cached after first run)
- **Processing Service**: 2-7GB depending on document size
- **Database**: Varies with document count and chunk size

### Processing Time
- **Small PDFs** (< 10 pages): 30-60 seconds
- **Medium PDFs** (10-50 pages): 2-5 minutes
- **Large PDFs** (50+ pages): 5-15 minutes

### Optimization Tips
- Use connection pooling for database
- Implement caching for frequent queries
- Batch embedding generation when possible
- Use appropriate chunk sizes for your use case

## ğŸ”’ Security Considerations

### API Keys
- Store in environment variables only
- Never commit to version control
- Use different keys for development/production

### File Upload
- Validate file types and sizes
- Sanitize filenames
- Implement rate limiting

### Database
- Use connection strings with credentials
- Implement proper access controls
- Regular security updates

## ğŸ§ª Testing Strategy

### Unit Tests
- Test individual functions and classes
- Mock external dependencies
- Test error handling paths

### Integration Tests
- Test API endpoints
- Test database operations
- Test queue processing

### End-to-End Tests
- Test complete user workflows
- Test file upload and processing
- Test chat functionality

## ğŸ“ˆ Monitoring and Observability

### Health Checks
- API server health endpoint
- Processing service health endpoint
- Database connectivity checks

### Metrics
- Processing time per document
- Queue depth and processing rate
- Database query performance
- Memory and CPU usage

### Logging
- Structured logging with timestamps
- Error tracking and alerting
- Performance metrics logging

## ğŸš€ Deployment Considerations

### Production Setup
- Use production-grade database
- Implement proper backup strategies
- Set up monitoring and alerting
- Configure SSL/TLS certificates

### Scaling
- Horizontal scaling of services
- Database read replicas
- Load balancing for API servers
- Queue partitioning for processing

### Maintenance
- Regular database maintenance
- Model updates and migrations
- Security patches and updates
- Performance optimization

## ğŸ“š Learning Opportunities

This project demonstrates:
- **Microservices Architecture**: Service separation and communication
- **RAG Implementation**: Vector search and context retrieval
- **Async Processing**: Message queues and background jobs
- **Real-time Communication**: WebSocket implementation
- **Container Orchestration**: Docker Compose
- **Vector Databases**: PostgreSQL with pgvector
- **LLM Integration**: Multiple provider support
- **TypeScript**: Strong typing and interfaces
- **Python**: FastAPI and async processing
- **React**: Modern hooks and component patterns

## ğŸ¤ Contributing Guidelines

### Code Style
- Follow TypeScript best practices
- Use meaningful variable and function names
- Add proper error handling
- Include type annotations

### Documentation
- Update README for new features
- Add inline code comments
- Document API changes
- Update this agents.md file

### Testing
- Add tests for new features
- Test error scenarios
- Verify integration points
- Test performance implications
